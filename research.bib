
@article{chen_gpt-3_2021,
	title = {{GPT}-3 {Displays} {Lower} {Social} {Intelligence} {Than} {Human} {Children}},
	url = {https://e674ea74-b63e-4ab8-9c5c-e52c255014ca.usrfiles.com/ugd/e674ea_52602e8a4b4041008a83ef76c6a98970.pdf},
	abstract = {Generative Pre-trained Transformer 3 (GPT-3) [1] is a neural language model with broad language capabilities. It achieves state-of-theart or human-level performance in translation, question answering, and creative writing tasks. It is deployed in commercial products such as
companion chatbots [2], however, the existing benchmarks do not specifically test for its level of social intelligence. In this project, we explore
GPT-3’s social intelligence by adapting batteries and human baselines from psychology research. We found that: 1) In non-social control questions,
GPT-3 performed at near-human level. 2) In social questions, it scored lower than both children and adults. 3) GPT-3’s performance did not significantly increase even when similar example’s solutions are provided.},
	journal = {National Collegiate Research Conference, Harvard University},
	author = {Chen, Yifu},
	month = jan,
	year = {2021},
}

@article{dragojlovic_estimating_2020,
	title = {Estimating {Patient} {Preferences} for {Drug} {Therapies} from {Online} {Text} {Using} {Aspect}-{Based} {Sentiment} {Analysis}},
	url = {https://e674ea74-b63e-4ab8-9c5c-e52c255014ca.usrfiles.com/ugd/e674ea_0043613d79a74a2b9d5f5ba794228e15.pdf},
	abstract = {Understanding patient preferences for drug therapies can help to inform drug development, reimbursement decisions by insurers, and shared decision-making by patients and clinicians. Typically, preferences are elicited using lengthy and costly studies involving qualitative analysis of interviews and focus groups along with complex stated preference surveys. Our objective is to develop a more efficient procedure that allows users such as patients or domain experts (eg. drug developers) to assess patient preferences semiautomatically from online patient-generated text using a weakly supervised aspect-based sentiment analysis pipeline. To facilitate users’ ability to generate new insights on patient preferences using our system, we present the design of two interfaces: one designed to suit patients’ requirements, the other for domain experts. This will enable both user groups to conduct frequent and adaptable assessments of patient preferences for drug therapies over-time.},
	journal = {Society for Medical Decision Making},
	author = {Dragojlovic, Nick and Johnson, David and Kopac, Nicola and Chen, Yifu and Lenzen, Marilyn and Huray, Sarah and Borle, Kennedy and Pollard, Samantha and George, Amy and Regier, Dean and Harrison, Mark and Carenini, Giuseppe and Ng, Raymond and Lynd, Larry},
	month = oct,
	year = {2020},
}

@article{zou_accuracy_2021,
	title = {Accuracy of {Natural} {Language} {Processing} in {Breast} {Cancer} {Outcomes} {Research} – {A} {Pilot} {Study} for {Algorithm} {Development} and {Validation} in {Synoptic} {Reports}},
	url = {https://e674ea74-b63e-4ab8-9c5c-e52c255014ca.usrfiles.com/ugd/e674ea_0f8cf85fc3824759929d379e90127675.pdf},
	abstract = {ABSTRACT: Purpose
Health outcomes research demands large volumes of data derived from various sources and phases of clinical care. This study aimed to develop a Natural Language Processing (NLP) algorithm for synoptic (structured) data and test its accuracy in automated data extraction from health records of immediate breast reconstruction (IBR) patients.

Methods
With ethics approval, a retrospective review of patients undergoing IBR from 2018-2019 was completed and data extracted for salient variables in outcomes research. Standardized data collection for input variables from clinical pathology reports, totalling 38 clinicalvariables, formed testing and validation samples. Two independent reviewers extracted data from 100 pathology reports, containing structured and unstructured data. Gold standard was defined as human extracted data, with quality assured by third reviewer, and kappa analysis was completed for inter-rater reliability. An NLP algorithm was developed to extract 38 identical fields of interest (FoIs) from each report. Accuracy of NLP algorithm automatic data
extraction was evaluated against manual human extraction.

Results
A total of 1938 inputs was used for testing and validating the NLP algorithm. Human data extraction had excellent inter-rater reliability (k = 0.88) and 2.3\% error rate (n=45 inaccuracies) before correction by third reviewer. Development of a rule-based NLP computer algorithm was achieved following pre-processing of records with optical character recognition and training based on FoI. Mean accuracy of the algorithm was 96.4\% (SD 4.3\%) across all 38 FoIs. In comparison with human-extracted data prior to third reviewer corrections, 12 FoIs displayed 100.0\% accuracy, indicating computer-extracted values were a superset of the human-extracted values.

Conclusions
High accuracy in automated data extraction was achieved with applied NLP algorithm and will be further optimized with data sampling and variability.},
	journal = {Canadian Society of Plastic Surgeons Annual Meeting},
	author = {Zou, Vito and Chen, Yifu and Yuan, Shawn and Hollander, Zsuzsanna and Ng, Raymond and Isaac, Kathryn},
	month = jun,
	year = {2021},
}

@article{chen_evaluating_2021,
	title = {Evaluating {Social} {Intelligence} in {NLP} {Models} with {Theory} of {Mind} {Stories} ({ToMS}): {A} {New} {Challenging} {Benchmark}},
	url = {https://e674ea74-b63e-4ab8-9c5c-e52c255014ca.usrfiles.com/ugd/e674ea_b2d54f86d44a48e2a3fcf3ebc7216fb4.pdf},
	abstract = {Neural natural language processing (NLP) models, mostly Transformers, recently achieved higher-than-human level performance on multiple tasks such as reading comprehension and translation. However, the existing NLP benchmarks rarely test for social reasoning, which is a significant facet of human abilities. To quantify the level of social intelligence in NLP models, we adapted and expanded psychology batteries to construct the Theory of Mind Stories (ToMS) benchmark. In contrast to normal reading comprehension, ToMS poses a unique challenge of inferring the unobservable mental state in humans.  We evaluated several state-of-the-art NLP models and reported results.  We also made this benchmark open-source with the hope to assist future research and development of human-centric NLP models.},
	journal = {Multidisciplinary Undergraduate Research Conference},
	author = {Chen, Yifu},
	month = mar,
	year = {2021},
}

@article{chen_investigating_2020,
	title = {Investigating {People}’s {Attitude} {Towards} {Utilizing} {Autonomous} {Vehicles}},
	abstract = {Technological initiatives are influencing the way we live. A wide range of benefits is expected from Autonomous vehicles (AVs) technology in terms of traffic safety. Also, it can significantly improve the mobility of individuals who are not able to drive, such as children, the elderly and the disabled. As suggested by survey analysis in previous recent studies, the early AVs adopters are likely to be the young, students, educated individuals who spend longer time in vehicles. In this study, a questionnaire of twenty eight questions was crafted to measure user preference regarding AVs and how likely this technology would be popularly accepted in a short time. Survey questions were distributed electronically to young adults, most of whom live in Canada or Singapore. Sixty-three individuals were willing to participate in this research. The obtained responses were then analyzed and visualized using Python and Tableau platforms. Based on this, quantitative observations were done by calculating the correlation score between each opinion. Results demonstrated that individuals who: 1) are more receptive to new products and technologies, 2) prefer not to drive, or 3) have a greater understanding of AVs, are more likely to demonstrate a pro-AVs attitude. It was also observed that young people are willing to utilize AVs to park or get groceries for them. On the other hand, respondents showed a lack of trust in AVs in terms of safety. The majority of participants believe AVs will be safe enough within the next ten years.},
	journal = {Multidisciplinary Undergraduate Research Conference},
	author = {Chen, Yifu and Nicholas, Teh and Xia, Manvis and Passant Reyad},
	month = mar,
	year = {2020},
}

@article{schaetzen_grid-based_2021,
	title = {A {Grid}-{Based} {Smoke} {Detection} {Convolutional} {Neural} {Network} {Model} on {Wildfire} {Cameras}},
	abstract = {With climate change effects projected to intensify over the next century, the steady increase in destructive wildfires has been a major cause for concern. While a major advancement in developing large-scale fire monitoring services was made with the establishment of ALERTWildfire, it has yet to adopt an automated smoke detection system to alleviate the task of manually monitoring live-feed footage for wildfires. To address this, we present a multi-label image classifier to predict forest fire smoke based on Pan-Tilt-Zoom (PTZ) image data. Our approach is to divide the image into a KxK grid and predict the cells of the grid containing smoke. We achieve 3 − 4 \% improvement over baseline for a 4 x 4 mesh and empirically establish upper bounds on grid size resolution. As a secondary contribution, we release the first smoke-annotated video dataset which consists of 139 hours of footage from PTZ cameras across 678 videos. We hope the public release of our dataset will accelerate future research efforts in large-scale automatic wildfire monitoring.},
	journal = {(manuscript in preparation)},
	author = {Schaetzen, Rodrige and Menoni, Raphael and Chen, Yifu and Hasani, Drijon},
	year = {2021},
}

@article{chen_automated_2021,
	title = {Automated {Medical} {Chart} {Review} for {Breast} {Cancer} {Outcomes} {Research}: {A} {Novel} {Natural} {Language} {Processing} {Extraction} {System}},
	abstract = {Background: Manually extracted data points from health records are collated on an institutional, provincial, and national level to facilitate clinical research. However, the labour-intensive clinical chart review process puts an increasing burden on healthcare systems. Therefore, an automated information extraction system is needed to ensure the timeliness and scalability of research data.

Methods: We used a dataset of 100 synoptic operative and 100 pathology reports, evenly split into 50 reports in training and validation sets for each type. The training set guided our development of a Natural Language Processing (NLP) extraction system, which accepts scanned images of operative and pathology reports. The system uses a combination of rule-based and transfer learning methods to extract numeric encodings from text. We also developed visualization tools to compare the manual and automated extractions.

Findings: A validation set of 50 operative and 50 pathology reports are used to compare the extraction accuracies between an MD student and the NLP system. An MD student yielded 92.1\% (operative) and 99.8\% (pathology) accuracies, while the NLP system achieved 91.9\% (operative) and 96.0\% (pathology) accuracies. 

Interpretation: The NLP system achieves near-human-level accuracy in both operative and pathology reports. The results support the deployment of this NLP system in production settings. Its use cases include 1) substituting human chart reviewers, 2) facilitating human reviewers through encoding recommendations, and 3) measuring the accuracy of human extractions.},
	journal = {(manuscript submmited to The Lancet: Digital Health)},
	author = {Chen, Yifu and Hao, Lucy and Zou, Vito and Hollander, Zsuzsanna and Ng, Raymond and Isaac, Kathryn},
	year = {2021},
}
